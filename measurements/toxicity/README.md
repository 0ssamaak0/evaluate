---
title: Toxicity
emoji: ðŸ¤—
colorFrom: blue
colorTo: red
sdk: gradio
sdk_version: 3.0.2
app_file: app.py
pinned: false
tags:
- evaluate
- metric
description: >-
The toxicity metric aims to quantify the toxicity of the input texts using a hate speech classification model trained for the task
---

# Metric Card for Toxicity

## Metric description


## How to use

Args:
    `predictions` (list of str): prediction/candidate sentences
    `model` (optional): the model to be used for measuring toxicity. The default is [roberta-hate-speech-dynabench-r4](https://huggingface.co/facebook/roberta-hate-speech-dynabench-r4-target)
        NOTE: The model has to be loadable using the AutoModelForSequenceClassification function. For more information, see:
                https://huggingface.co/docs/transformers/master/en/model_doc/auto#transformers.AutoModelForSequenceClassification
    `toxic_label` (optional): the toxic label that you want to detect, depending on the labels that the model has been trained on.
        This can be found using the `id2label` function, e.g.:
            ```
            >>> model = AutoModelForSequenceClassification.from_pretrained("DaNLP/da-electra-hatespeech-detection")
            >>> model.config.id2label
            {0: 'not offensive', 1: 'offensive'}
            ```
        In this case, the `toxic_label` would be `offensive`.
    `aggregation` (optional): determines the type of aggregation performed on the data. If set to `None`, the scores for each prediction are returned.
     Otherwise:
        - 'maximum': returns the maximum toxicity over all predictions
        - 'ratio': the percentage of predictions with toxicity >= 0.5.




## Output values

    `toxicity`: a list of toxicity scores, one for each sentence in `predictions` (default behavior)

    `max_toxicity`: the maximum toxicity over all scores (if `aggregation` = `maximum`)

    `toxicity_ratio`": the percentage of predictions with toxicity >= 0.5 (if `aggregation` = `ratio`)


### Values from popular papers


## Examples
    Example 1 (default behavior):
  ```
        >>> toxicity = evaluate.load("toxicity", module_type="metric")
        >>> input_texts = ["she is very mean", "he is a douchebag", "you're ugly"]
        >>> results = toxicity.compute(predictions=input_texts)
        >>> print(results)
        {'toxicity': [0.00013419731112662703, 0.856372594833374, 0.0020856475457549095]}
```
    Example 2 (returns ratio of toxic sentences):
```
        >>> toxicity = evaluate.load("toxicity", module_type="metric")
        >>> input_texts = ["she is very mean", "he is a douchebag", "you're ugly"]
        >>> results = toxicity.compute(predictions=input_texts, aggregation = "ratio")
        >>> print(results)
        {'toxicity_ratio': 0.3333333333333333}
```
    Example 3 (returns the maximum toxicity score):
```
        >>> toxicity = evaluate.load("toxicity", module_type="metric")
        >>> input_texts = ["she is very mean", "he is a douchebag", "you're ugly"]
        >>> results = toxicity.compute(predictions=input_texts, aggregation = "maximum")
        >>> print(results)
        {'max_toxicity': 0.856372594833374}
```
    Example 4 (uses a custom model):
```
        >>> toxicity = evaluate.load("toxicity", module_type="metric")
        >>> input_texts = ["she is very mean", "he is a douchebag", "you're ugly"]
        >>> results = toxicity.compute(predictions=input_texts, model= 'DaNLP/da-electra-hatespeech-detection', toxic_label='offensive')
        >>> print(results)
        {'toxicity': [0.004464445170015097, 0.020320769399404526, 0.01239820383489132]}
```

## Citation

```bibtex
@inproceedings{vidgen2021lftw,
  title={Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection},
  author={Bertie Vidgen and Tristan Thrush and Zeerak Waseem and Douwe Kiela},
  booktitle={ACL},
  year={2021}
}
```

```bibtex
@article{gehman2020realtoxicityprompts,
  title={Realtoxicityprompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={arXiv preprint arXiv:2009.11462},
  year={2020}
}

```

## Further References
